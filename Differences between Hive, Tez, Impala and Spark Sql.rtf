{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 ArialMT;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;\csgray\c100000;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww18020\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl460\partightenfactor0

\f0\fs40 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Differences between Hive, Tez, Impala and Spark Sql
\f1\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
- Every execution engine has same type of sql\'92s but what is the difference\
- Mainly we need to know between hive and spark\
- Tez and impala is between HortonWorks and Cloudera\
\

\b Hive
\b0  \
- Supports ASCII standards \
- Supports almost all the type of SQL like select, from, where, join, join multiple tables, group by, order by, etc\
- When you run a hive query, it executes a MapReduce + YARN execution framework. ( It is configured like that)\
- Hive metastore is build using the databases like mysql \
- The megastore has the table name, column name, rest of the metadata related to the table\
- What ever execution engine it might be, Hive, Tex, Impala, Spark SQL with HoveContext. It connects to the megastore to get the metadata of the tables\
- Actual Data is stored in HDFS\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 - To see the meta data of the table -> DESCRIBE FORMATTED orders;\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 - When the Job is ran, It runs a Map Reduce job. When there are multiple MR jobs running for one query, the output of the first reducer job will be input to the Mapper for the next MR job.\
\

\b TEZ
\b0 \
- Tez developers thought that that communication is not necessary. When a query is executed using multiple MR Jobs, The output of one MR job need not be ported into mapper of the 2nd MR Job. There can be some redundancy and if that can be eliminated the performance can be better. \
- This is where TEZ DAG\'92s come into picture \
- set hive.execution.engine=tez;\
\

\b IMPALA
\b0 \
- Impala instead of YARN and MR, It uses a daemon process called Impalad for execution \
- Impala has 4 Daemon processes and Master processes\
	-> Impala Catalog Server\
	-> Impala Daemon\
	-> Impala Llama ApplicationMaster\
	-> Impala StateStore\
- Like YARN has ResourseManager , Impala has Catalog Server and StateStore\
- The processes which actually perform the heavy lifting are Impala Daemon process. This will run on all the slaves where the data nodes are running. \
- These Impala D processes by pass the Map Reduce processing \
\

\b SPARK
\b0 \
- Spark is basically executed by Spark executors. These executors can be native or can be controlled by YARN.\
- This also bypasses MR framework completely instead it uses spark execution framework to execute. \
\
\

\i \ul Compilation Steps are similar across all the distributions.
\i0 \ulnone \
- When you write a query, it will check for the for clause and see if all the tables exists or not. To do this is needs a metastore. This MetaStore is in RDBMS\
- So the first step is to check is the query is syntactically correct, Then is the tables are existing in the hive metastore \
- Once it reaches the metastore and fetches the metadata of these tables, it understands the location of the files in HDFS\
- Then depending upon the framework, this query will be compiled into a executable application.  \
- If it is 
\i Hive or Tez
\i0 , it uses MR Api\'92s itself to compile the query into a java program and that java program will be compiled into a executable JAR and that JAR will be submitted as an application in the cluster. \
 - If it is Impala then it generates the code in impala compatible language and then compiled by impala and that will be submitted as a job in impala\
- If it is Spark, It converts into spark based application.\
\

\i \ul Execution
\i0 \ulnone \
- Hive uses MapReduce which is heavy I/O intensive compared to other frameworks. \
- Tez uses MapReduce for API\'92s and for execution it uses YARN directly \
- Impala uses MapReduce for API\'92s and for execution it uses its Daemons.\
- Spark SQL uses Spark based MR and executes in-memory \
\

\i \ul Suitable For
\i0 \ulnone  \
- Hive is suitable for very large batches like for several minutes to hours or days too \
- Not effective for large batches. But if you need to run some reports in interactive fashion by connecting to hadoop cluster then tez and impala are effective solutions compared to HIVE\
- Spark-SQL, the advantage of this is it runs In-Memory. Hence it is very fast for interactive reporting and also for medium size batches \
\
					
\i\b Hive
\i0\b0 			
\i\b Tez
\i0\b0 		
\i\b Impala
\i0\b0 	
\i\b Spark SQL
\i0\b0 \

\i\b Initiated By
\i0\b0 				FB, Apache		Hortonworks	Cloudera	Databricks\

\i\b Execution Framework
\i0\b0 		YARN+MR2		YARN		Impala D	Spark Executors\

\i\b Compilation
\i0\b0 				MapReduce		Hybrid		Hybrid		In-Memory\

\i\b Suitable For
\i0\b0 				LargeBatches	Interactive	Interactive	Interactive and medium sized batches\
\
\
\
\
}